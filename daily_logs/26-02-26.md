# Date 26-02-26
Today I continued working on the Performance Ninja playlist, covering vectorization, function inlining, compiler intrinsics, loop tiling, branch optimization, and false sharing. Along with watching the videos, I practiced the corresponding implementations from the repository and analyzed the benchmark outputs after each optimization.

In vectorization, I initially found it difficult to clearly understand how a normal scalar loop gets converted into SIMD instructions at the assembly level. For example, in a simple array addition loop, I knew logically that elements are added one by one, but I struggled to visualize how AVX registers process multiple elements in parallel using 256-bit registers. By compiling with higher optimization flags and examining how data is aligned and accessed, I understood how the compiler groups operations and executes them using SIMD instructions to improve throughput.

In function inlining, I was confused about when the compiler decides to inline a function and how it impacts real performance. For example, in small frequently called helper functions inside tight loops, I observed that removing the function call overhead improves execution time. However, I also understood that excessive inlining can increase binary size and potentially affect instruction cache behavior. Running benchmarks helped me see the measurable difference.

In loop tiling, the main difficulty was understanding cache locality in a practical way. When working with matrix traversal examples, I initially did not grasp why simply changing loop ordering affects performance significantly. After analyzing how data is stored in row-major order and how tiled blocks fit better into cache lines, I understood how restructuring loops reduces cache misses and improves memory access efficiency.

In branch optimization, I struggled to understand how unpredictable branches reduce CPU efficiency. Through the example of conditional statements inside tight loops, I observed how replacing branches with lookup tables or branchless techniques improves consistency and performance. In the false sharing example, I initially found it hard to visualize how multiple threads writing to nearby memory locations can cause performance degradation. After studying the example with shared counters, I understood how cache line contention happens and how adding padding between variables prevents multiple threads from competing for the same cache line, thereby improving multi-threaded performance.