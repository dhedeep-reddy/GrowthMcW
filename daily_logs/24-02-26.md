# Date: 24-02-26

Yesterday I focused deeply on computer architecture concepts and performance optimization, especially SIMD programming. My learning shifted more toward understanding how the CPU actually executes instructions rather than just writing high level AI or ML code.

I started by clarifying the concept of Instruction Set Architecture. I explored whether x86 and ARM are ISA types or CPU architectures and understood that they are ISA families. ISA defines the instructions, registers, data types, memory behavior, and acts as the interface between software and hardware. I also learned the difference between ISA and microarchitecture. While companies like Intel and AMD may both implement the x86 ISA, their internal CPU designs such as pipelines, execution units, cache structures, and branch predictors can be different. That internal implementation is called microarchitecture.

A major part of my learning was around SIMD programming. I understood that SIMD stands for Single Instruction Multiple Data, meaning a single instruction operates on multiple data elements simultaneously. I explored why SIMD is used instead of MIMD in certain scenarios. SIMD works efficiently for vector based operations like image processing, signal processing, and machine learning computations. It operates inside a single core using vector execution units, whereas MIMD involves multiple cores running different instructions. SIMD provides lower overhead and better efficiency for data parallel workloads.

I also studied AVX and understood what the 256 in AVX 256 represents. AVX stands for Advanced Vector Extensions and extends the x86 ISA with vector capabilities. The number 256 refers to the width of the vector registers in bits. A 256 bit register can process multiple values at once, such as eight 32 bit floating point numbers or four 64 bit doubles in a single instruction. This increases data parallelism significantly.

I learned about intrinsic functions in C++. Intrinsics are special compiler provided functions that directly map to CPU instructions. Instead of writing assembly code, I can use intrinsics in C++ to generate SIMD instructions like AVX. This gives me more control over performance while still working in a high level language.

I also clarified the concept of additional ALUs shown in processor diagrams. Modern CPUs contain scalar ALUs as well as vector execution units. The additional ALUs in the diagrams represent vector units that execute SIMD instructions such as AVX operations.

In addition, I revised C++ data type sizes and their bit widths, which is important for understanding how many elements can fit into a SIMD register. I also analyzed the compiler command g++ simd.cpp -O3 -mavx2. I understood that O3 enables aggressive optimization and mavx2 allows the compiler to generate AVX2 instructions. Without that flag, SIMD instructions would not be generated.

Finally, I explored the concept of performance tuning. I understood that performance tuning involves improving execution speed using techniques such as SIMD, compiler optimizations, cache aware programming, loop optimizations, and efficient memory access patterns.

Overall, yesterday marked a strong shift in my learning toward low level system understanding and CPU level optimization, which aligns well with high performance computing and ML systems engineering roles.